# ---- Model definitions ----

# You can either specify models inside the engine definition, or specify them first.
# Putting them first like this allows them to be used multiple times while only loading 
# them off disk once, speeding up loading engines that share model files

# Here we define the base v1.5 Stable Diffusion pipeline model. Model definitions must
# have a unqiue model_id which is how they are referenced later.
- model_id: "stable-diffusion-v1-5-base"
  # This is the HuggingFace model ID to load from if the local version isn't found.
  # You can leave it out, but then the model will fail to load if the local files don't exist
  model: "runwayml/stable-diffusion-v1-5"
  # If loading from HuggingFace, for this model we need to provide an authorization token
  use_auth_token: True
  # This is a path (relative or absolute) to look for a local copy of the fp32 model.
  # You can leave it out if you want to only load from HuggingFace, or if you only use fp16 mode
  local_model: "./stable-diffusion-v1-5"
  # This is a path (relative or absolute) to look for a local copy of the fp16 model
  # You can leave it out if you want to only load from HuggingFace, or if you only use fp32 mode
  local_model_fp16: "./stable-diffusion-v1-5-fp16"

# Here we define the inpainting model
- model_id: "stable-diffusion-inpainting"
  # We only use the unet from this model, so skip loading anything else
  whitelist: "unet"
  # Everything else is this model you've seen above
  model: "runwayml/stable-diffusion-inpainting"
  use_auth_token: True
  local_model: "./stable-diffusion-inpainting"
  local_model_fp16: "./stable-diffusion-inpainting-fp16"

# Here we define the Stability VAE model
- model_id: "stability-vae-ema"
  # This is only a vae model, not a pipeline, so we must tell the server that
  type: "vae"
  # There isn't an fp16 version of the model available, so always load the fp32 version.
  # The server will convert it to fp16 if needed.
  has_fp16: False
  # The HuggingFace model stays the same if has_fp16 is False
  model: "stabilityai/sd-vae-ft-ema"
  # But if you provide a local model, you can only provide the fp32 version
  local_model: "./sd-vae-ft-ema"

# Here we define a model that has the base SD V1.5 pipeline with two other models 
# loaded over top, as we often use this combination later.
- model_id: "stable-diffusion-v1-5-full"
  # Rather than specifying a HuggingFace model, here we load from the previously
  # defined model stable-diffusion-v1-5-base.
  model: "@stable-diffusion-v1-5-base"
  # We then provide two additional models to load into the pipeline.
  overrides:
    # We load the inpaint_unet here (the pipeline otherwise wouldn't have one.)
    inpaint_unet:
      # Because we're using a model from a whole pipeline, we need to specify to use the unet.
      model: "@stable-diffusion-inpainting/unet"
    # We override the vae with a different one here.
    vae: 
      # Because we're using a simple model, we just reference it directly.
      model: "@stability-vae-ema"

# Here are two CLIP models.
- model_id: "laion-clip-h"
  # Clip models can be loaded as clip_model or feature_extractor. By using a type of clip
  # a pipeline is created with just clip_model and feature_extractor models.
  # You _could_ specify clip_model or feature extractor here to just load that type of model
  type: "clip"
  model: "laion/CLIP-ViT-H-14-laion2B-s32B-b79K"
  has_fp16: False
- model_id: "laion-clip-b"
  type: "clip"
  model: "laion/CLIP-ViT-B-32-laion2B-s34B-b79K"
  has_fp16: False

# And finally, the Waifu Diffusion V1.3 pipeline. By now everything should make sense.
- model_id: "waifu-diffusion-v1-3-base"
  model: "hakurei/waifu-diffusion"
  local_model: "./waifu-diffusion-v1-3"
  local_model_fp16: "./waifu-diffusion-v1-3-fp16"
  use_auth_token: False

# ---- Engine definitions ----

# Now we define a set of engines that will handle requests. These expect
# to be passed a classname to use to actually build the engine with, a model
# (either from the definitions above or just declared inline) and maybe some options

# This is the main Stable Diffusion V1.5 model
- id: "stable-diffusion-v1-5"
  # You can set enabled to False to temporarily prevent it from being loaded
  enabled: True
  # You can set visible to False to load the engine but prevent it from showing up in the API list
  visible: True
  # This name can be used by User Interfaces
  name: "Stable Diffusion V1.5"
  # This description can be used by User Interfaces
  description: "Stable Diffusion using the RunwayML model and our Unified pipeline"
  # This class is used to actually handle the generation.
  class: "UnifiedPipeline"
  # You can set various options on each engine. In this case we're enabling xformers (if available)
  options:
    xformers: True
  # And finally we set the model to use - in this case we're just referencing an already-loaded model
  model: "@stable-diffusion-v1-5-full"

# This is the Stable Diffusion V1.5 model with CLIP guidance enabled. This configuration needs quite
# a lot of VRAM, but is faster and produces better results if you have enough.
- id: "stable-diffusion-v1-5-clip"
  enabled: True
  visible: True
  name: "Stable Diffusion V1.5 CLIP guided"
  description: "Stable Diffusion using the RunwayML model, CLIP guidance and our Unified pipeline"
  class: "UnifiedPipeline"
  options:
    clip:
      approx_cutouts: 2
      vae_cutouts: 2
  model: "@stable-diffusion-v1-5-full"
  overrides:
    # Just like in models, clip means both clip_model and feature_extractor
    clip: "@laion-clip-h"

# This is the Stable Diffusion V1.5 model with CLIP guidance enabled. This configuration needs much less
# VRAM (6GB should be sufficient in fp16 mode if xformers is available), but is slower and slightly less accruate
- id: "stable-diffusion-v1-5-clip-small"
  enabled: True
  visible: True
  name: "Stable Diffusion V1.5 CLIP guided"
  description: "Stable Diffusion using the RunwayML model, CLIP guidance (limited memory version) and our Unified pipeline"
  class: "UnifiedPipeline"
  options:
    xformers: True
    clip:
      approx_cutouts: 2
      vae_cutouts: 0
  model: "@stable-diffusion-v1-5-full"
  overrides:
    clip: "@laion-clip-b"

# This is the basic Waifu Diffusion model
- id: "waifu-diffusion-v1-3"
  enabled: True
  visible: True
  name: "Waifu Diffusion V1.3"
  description: "Stable Diffusion using the Hakurei Waifu Diffusion model and our Unified pipeline"
  class: "UnifiedPipeline"
  options:
    xformers: True
  model: "@waifu-diffusion-v1-3-base"

# This is the Waifu Diffusion model, but with Stable Diffusion's inpainting grafted in. This generally
# gives better results than the model-independant inpainting method, but has it's own oddities
- id: "waifu-diffusion-v1-3-grafted"
  enabled: True
  visible: True
  name: "Waifu Diffusion V1.3 Grafted"
  description: "Waifu Diffusion with grafted inpainting"
  class: "UnifiedPipeline"
  options:
    grafted_inpaint: True
    graft_factor: 0.8
    xformers: True
  model: "@waifu-diffusion-v1-3-base"
  overrides:
    inpaint_unet: "@stable-diffusion-inpainting/unet"

# This is the Waifu Diffusion model with SD grafted inpaint and CLIP guidance (high vram version)
- id: "waifu-diffusion-v1-3-grafted-clip"
  enabled: True
  visible: True
  name: "Waifu Diffusion V1.3 Grafted"
  description: "Waifu Diffusion with grafted inpainting and CLIP guidance"
  class: "UnifiedPipeline"
  options:
    grafted_inpaint: True
    graft_factor: 0.8
    clip:
      approx_cutouts: 2
      vae_cutouts: 2
  model: "@waifu-diffusion-v1-3-base"
  overrides:
    inpaint_unet: "@stable-diffusion-inpainting/unet"
    clip: "@laion-clip-b"

# And finally, here is the unmodified Stable Diffusion V1.4 model, showing how to list
# the model inline
- id: "stable-diffusion-v1-4"
  enabled: True
  visible: True
  name: "Stable Diffusion V1.4"
  description: "Stable Diffusion using the CompVis model and our Unified pipeline"
  class: "UnifiedPipeline"
  model: "CompVis/stable-diffusion-v1-4"
  local_model: "./stable-diffusion-v1-4"
  local_model_fp16: "./stable-diffusion-v1-4-fp16"
  use_auth_token: True
